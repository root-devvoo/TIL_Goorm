# 구름 쿠버네티스 과정 5기 54일차\_20220927

## Kubernetes 정리 (2)

**서비스** 

- 파드에 대한 네트워크 연결을 위해 사용

- 내부용 고정 IP 주소 사용을 위해 (ClusterIP)

- 외부에서 접근할 수 있는 네트워크 연결 제공

  → 각 노드의 IP : NodePort

  별도의 장치 : LoadBalancer

- 파드에서 외부로 연결 시 사용 (ExternalName)

**clusterIP + readiness**

1. clusterIP 서비스를 배포 후 확인 (svc, ep)

2. deployment 배포 (서비스와 연결) 후 확인

   → 이미지를 nginx 로 해서 접속 확인 (POD / SVC)

3. 파드 배포 및 접속 확인

   - 교재 p.201 에서 `nicolaka/netshoot` 이라는 이미지 사용
   - 파드 내부에서 `curl` 로 접속 확인

4. clusterIP 서비스 배포

5. readinessProbe 를 설정한 deployment 를 배포 및 확인

   - exec 방식으로 검사

   - sleep / touch / cat 순서로 약 1분 정도 후에 준비완료

   - endpoint / pod 상태 함께 확인

     > `kubectl get ep`로 endpoint 확인

6. 서비스와 디플로이먼트를 하나의 파일에 작성 후 배포

**NodePort**

1. 기존에 배포해둔 파드에 연결 후 확인
2. 컨트롤플레인/노드 각각의 주소로 접속 (윈도우)
3. clusterIP 주소로도 접속 확인 (가상머신)

**LoadBalancer**

1. 기존에 배포해둔 파드에 연결 후 확인
2. External-IP 주소로 접속
3. 노드/clusterIP 주소로 확인 (윈도우/가상머신)

**ExternalName**

1. 별도의 가상머신 / 컨테이너를 사용해서 웹 서버 구성

2. `/etc/hosts` (DNS 대용) 파일에 IP/DNS 등록

   > DNS 참고 링크 : <https://v1-23.docs.kubernetes.io/ko/docs/concepts/services-networking/dns-pod-service/#%EC%84%9C%EB%B9%84%EC%8A%A4>

3. externalName 서비스 배포 시 해당 주소 설정

4. `nicolaka/netshoot` 이미지로 파드 생성 후 접속

**headless**

1. `c1t1d0s7/myweb` 이미지로 statefulset 배포
2. clusterIP 를 배포 및 연결
3. 연결 확인(svc,ep) 후 clusterIP 주소로 접속
4. 특정 pod의 주소로 접속 후 해당 pod를 삭제 후 확인
5. `c1t1d0s7/myweb` 이미지로 statefulset 로 배포
6. headless 를 배포 및 연결
7. 연결 확인(svc,ep) 후 서비스의 DNS로 접속
8. 특정 pod의 IP주소/DNS 로 접속 후 해당 pod를 삭제 후 확인

**kube-proxy**

- 쿠버네티스 환경에서 파드에 대한 네트워크 연결 구성 관리

- 서비스 생성 시 서비스와 파드의 연결

- 내부적인 구조가

  userspace → iptables → IPVS

  위와 변화 (성능 향상을 위해)

**Ingress**

> 참고 링크 : <https://v1-23.docs.kubernetes.io/ko/docs/concepts/services-networking/ingress/>

- 외부에서 파드 접근을 위해서 사용
- 하나의 인그레스를 이용해서 여러 서비스(파드)에 포워딩
- URL / 경로 기준으로 분산처리
- 버전에 따른 설정이 다를 수 있음 ( class / default )

1. deployment 를 2개 배포
   - nginx, c1t1d0s7/myweb (port=8080)
   - 이름 / 레이블 / 포트
2. NodePort 를 2개 배포
   - 각 deployment 에 하나씩 연결
   - selector / port 설정을 맞춰서
3. 서비스로 접속 확인
4. ingress 를 배포
   - host 항목에서 URL 주소를 2가지로 설정
   - 각 URL 별로 서로 다른 SVC로 연결
5. 접속 확인 (`/etc/hosts` 파일에 주소 설정 후)
6. ClusterIP 를 배포하고 Ingress 연결 (내부에서 확인하기 위함임... 원래는 NodePort로 하는게 맞음)

**레이블 / 셀렉터**

- 파드와 컨트롤러 / 파드와 서비스 의 연결을 위해
- 스케줄링 시 파드를 배치할 노드 선택 시에도 

**애너테이션**

- 필요한 정보를 저장해두는 용도
- 특정 기능을 가진 경우도 존재

1. 교재 p.250 에 있는 파일 참고해서 업데이트 정책 설정 실습

**컨피그맵 / 시크릿**

- 환경설정 등을 위해 별도의 변수/파일을 등록해두는 방식
- 컨피그맵 : 평문 , 시크릿 : 인코딩

1. 컨피그맵(시크릿) 배포
2. 파드 배포 시
   1. 값을 하나만 가져오기
   2. 값을 전체 가져오기
   3. 볼륨으로 연결해서 파일형식으로 사용

**파드 스케줄링**

- kube-scheduler 컴포넌트를 통해 파드를 배치

- 스케줄링 종류

  - 노드 네임 : 노드 이름이 일치하는 곳에 배포

  - 노드 셀렉터 : 노드에 레이블 설정

    → 해당하는 노드에 배치 (무조건적인 규칙)

    해당하는 노드가 없으면 Pending

  - 노드 어피니티 : 노드에 레이블 설정

    → 해당하는 노드에 배치 (강제/권장)

    prefer 의 경우에는 해당 노드가 없어도 배포

    


  ☞ 파드를 배치할 노드를 선택

  

  - 파드 어피니티 : 파드를 묶어서 / 흩어지게 배치

    예) WEB - DB 연계 배포

    web 디플로이먼트에서 안티어피니티 설정 후 배포

    db 디플로이먼트에서 web 파드에 대한 어피니티 설정<br>→ 서로 다른 노드마다 web/db 파드가 세트로 하나씩

    


  ☞ 각각의 파드에 대한 배치 선택

  

  - 테인트 : 노드에 설정하고 해당 노드를 제외한 배치

  - 톨러레이션 : 테인트에 대한 예외처리를 위해 사용

    

  - 커든 : 노드에 대한 배치 거부

    

  ☞ 노드에 대한 배치 거부 설정

  

- 드레인 : 노드를 지정해서 해당 노드에서 기존에 동작하던 파드들을 종료

  

  ☞ 노드에 대한 유지보수를 위해

<br>

스케줄링 실습 가이드

1. 노드 셀렉터와 노드 어피니티
   1. 특정 노드(`kube-node1`)에 `disktype=ssd` 항목을 설정
   2. 특정 노드(`kube-node2`)에 `disktype=hdd` 항목을 설정
   3. 디플로이먼트로 배포 (복제본3개)<br>→ nodeSelector / nodeAffinity (require/prefer)
   4. 1번 항목의 값도 hdd 로 변경 (해당하는 항목이 없도록)
   5. 디플로이먼트로 새로 배포
2. 파드 어피니티/안티어피니티
   1. 디플로이먼트로 배포할 때에 안티어피니티 설정<br>복제본 : 3개 / 4개 (노드개수보다 크게)
   2. 파드를 2번 배포<br>(첫번째 배포는 그냥 , 두번째는 어피니티 설정 후)
   3. 디플로이먼트 2종류로 배포<br>첫 번째는 안티어피니티를 설정 배포<br>두 번째는 어피니티 설정 배포 (안티어피니티)
3. 테인트/톨러레이션
   1. 파드를 배포
   2. 파드가 배포된 노드에 테인트 설정 (기본)
   3. 데몬셋으로 배포
   4. 디플로이먼트로 배포 (복제본 10개)
   5. 톨러레이션 설정 후 파드를 배포
4. 커든
   1. 테인트 실습처럼 순서대로 진행해보기
5. 드레인
   1. 파드가 배치된 특정 노드를 지정해서 실행
   2. 파드/데몬셋/디플로이먼트 각각의 상태 확인

**인증**

> ![image](https://user-images.githubusercontent.com/78403443/192459528-777ddcd9-0d7d-4b1a-b852-1d68b97c7063.png)
>
> 인증서는 값이 길다보니까 나타나지 않고, DATA-OMITTED 텍스트로만 보여짐.
>
> ![image](https://user-images.githubusercontent.com/78403443/192460720-4fd2da5b-c04b-4012-bc9f-4aa3d986c12b.png)
>
> 이런 절차로 인증 token 및 시크릿 확인
>
> ![image](https://user-images.githubusercontent.com/78403443/192461202-798c7272-3be3-4847-9f20-45efc5de6fb8.png)

**인증/인가**

- 작업 시 사용자에 대한 인증이 필요
- 해당 사용자에게 권한이 필요

인증을 위해

- 외부의 인증 서비스 : 일반 사용자

- 클러스터 내부적으로 인증 : 서비스 사용자

  → TLS 기반 / 토큰<BR>이때 토큰은 서비스 계정 생성 시 자동 생성

- 인증 시

  사용자 (인증서/토큰), 클러스터 (주소)<BR>→ `~/.kube/config` 파일에 등록

  - 해당 파일 구조
    - clusters : 관리할 대상(클러스터)
    - contexts : 대상-관리자
    - users : 관리자 (사용자)
    - current-context : 활성화

- 인가 (권한 설정)

  - 사용자 준비 (SA 생성)

  - 역할 - 일반롤(네임스페이스 범위) / 클러스터롤(전체)

  - 연결 - 롤바인딩 / 클러스터롤바인딩

    

    역할 설정 시<BR>대상 (그룹이름 + 리소스이름) / 동작

    

    연결 시<BR>롤이름 + 사용자이름

인증 관련 실습 가이드

1. 사용 가능한 사용자로 작업 확인

   - kubernetes-admin 사용자로 테스트
   - 리눅스에서 root 사용자로 명령어 사용 확인
   - 일반사용자 생성 후 `kubectl` 명령어 사용 가능하도록 설정

2. 기본 서비스 계정 (default) 으로 명령어 사용 허용

   1. default 네임스페이스에서만 파드 목록 확인 가능

   2. 전체 범위로 파드 목록 확인 가능

      

   → config 파일 수정 및 롤/롤바인딩 (편집기)

   

3. 새로운 계정 생성 후 설정

   1. SA 생성 및 토큰 확인

   2. config 에 등록 및 명령어 사용 확인

   3. 권한 설정 후 다시 확인 (컨트롤로 확인)

      

   → 명령어로 편집 작업 ( `kubectl config XXX` )

   

4. 새로운 가상머신 생성 및 kubectl 명령어 설치

   - config 파일 설정으로 외부의 클러스터가 관리 가능한지
