# 구름 쿠버네티스 과정 5기 56일차\_20220929

## Kubernetes 정리 (4)

쿠버네티스 볼륨

- 공유 (파드 안의 컨테이너 / 각 노드에서 파드)
- 영구저장 ( 파드를 삭제해도 데이터 유지)

종류

- emptyDir : 파드 안의 컨테이너끼리만 공유<br>→ 영구저장 X , 메모리 영역 사용 가능

- hostPath : 파드를 실행하는 특정 노드의 디렉토리를 공유<br>→ 호스트의 데이터를 공유 , 파드의 이동이 어려움

- nfs : NFS서버를 준비 (파드/시스템) 후 파드 볼륨 설정 시 바로 연결<br>→ 동일한 데이터를 각 파드가 공유

- PV / PVC / SC

  - 기본적으로 사용자(개발자)가 파드 생성 시 직접 볼륨 연결

  - 볼륨 연결 시 해당 스토리지에 대한 지식 필요

  - 사용자 편의를 위해서 스토리지 관리와 파드 관리를 분리

  - 사용자는 파드 생성 시 PVC 만 설정<br>→ PVC : 볼륨에 대한 크기/권한/종류

    스토리지 관리자가 PV 를 설정<br>→ PV : 실질적인 스토리지에 사용 공간 할당

    매번 요청에 대해 볼륨 생성이 어려움<br>→ SC : 사용자의 요청 시 볼륨을 자동으로 생성

       ☞ SC 를 이용해서 PVC 에 대한 PV 를 자동할당

쿠버네티스 네트워크

- 파드 하나 당 컨테이너가 하나 이상 존재<br>→ 파드 하나에 IP 주소 하나 할당<br>pause 컨테이너가 네트워크 관리 역할을 담당
- 멀티노드 환경<br>→ 파드/서비스 간에 내부적인 IP 주소를 통해 통신<br>모든 오브젝트를 동일한 네트워크 대역 으로 구성<br>CIDR 을 추가적으로 구성해서 노드 별로 구분

쿠버네티스 DNS

- 외부/내부 DNS 주소가 필요<br>→ 각각에 대한 DNS 서버가 필요

  

- 내부 DNS 서비스

  - 모든 파드/서비스 에 대해 각각 DNS 주소 할당
  - 파드 : IP.NAMESPACE.pod.CLUSTERNAME
  - 서비스 : SERVICENAME.NAMESPACE.svc.CLUSTERNAME

클러스터 내부 DNS 는 항상 coreDNS 로 요청 (기본)

- → 요청이 많아지면 부하증가<br>부하분산을 위해서 NodeLocalDNS 활용

- 동작방식
  1. 최초 파드가 DNS 주소 확인 필요
  2. NodeLocalDNS 에 주소 확인 요청
  3. NodeLocalDNS 에 주소정보가 없으면 coreDNS 로 요청
  4. coreDNS 에서 찾은 정보를 NodeLocalDNS 에 전달
  5. NodeLocalDNS 에서는 정보를 저장+파드에 전달
  6. 이후에 같은 정보를 요청하면 즉각적인 응답

---

**로깅과 모니터링** (교재 p.386 부터)

일반적인 시스템의 로그 관리

- 특정 디렉토리에 로그를 기록 (전체적으로)
- 데이터가 많아지거나 오래되면 logrotate 등의 도구로 순환

쿠버네티스의 로그 관리

- 환경이 여러 대의 시스템을 사용하는 클러스터 환경
- 여러 파드가 생성/삭제 되는 환경
- 로그를 무작정 저장하기에는 내용이 많음
- 각 시스템마다 따로 저장하면 관리/확인 어려움<br>→ 클러스터 레벨의 로그 관리 기능의 필요성 (추가구성)
- 기본적으로는 각 노드에서 로컬스토리지(디렉토리)에 저장<br>→ 컨테이너에 대한 로그 / 컴포넌트 등의 로그<br>문제점 : 직접 접속해서 확인해야 한다.
- kubectl 명령어로 로그 확인 가능<br>→ 명령어로 모든 노드에서 동작 중인 파드 확인 가능<br>문제점 : 파드가 동작하는 동안에만 확인 가능

필요한 부분

1. 클러스터 레벨 (여러 시스템의 로그를 한군데 모아서 확인)
2. 영구적인 저장 (별도의 스토리지에 모아서 저장)

Elastic Stack

구성 방식

1. EFK (교재)
2. ELK

구성요소

1. Elastic Search
   - 검색 도구<br>검색 기능 + 실시간 분석 + 인덱싱 \+ 저장<br>싱글노드방식 / 멀티노드방식
2. Logstash / Fluentd / Beat
   - 로그 수집 + 필터링<br>선택 시 고려사항 : 시스템 스펙, 버전관리
3. Kibana
   - 시각화 도구

> `logging/elastic.yaml` 편집
>
> ```bash
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: elasticsearch
>   namespace: elastic-stack
> spec:
>   replicas: 1
>   selector:
>     matchLabels:
>       app: elasticsearch
>   template:
>     metadata:
>       labels:
>         app: elasticsearch
>     spec:
>       containers:
>       - name: elasticsearch
>         image: elastic/elasticsearch:6.4.0
>         env:
>         - name: discovery.type
>           value: "single-node"
>         ports:
>         - containerPort: 9200
>         - containerPort: 9300
> 
> ---
> 
> apiVersion: v1
> kind: Service
> metadata:
>   name: elasticsearch-svc
>   namespace: elastic-stack
> spec:
>   ports:
>   - name: elasticsearch-rest
>     port: 9200
>     targetPort: 9200
>     nodePort: 30920
>   - name: elasticsearch-nodecom
>     port: 9300
>     targetPort: 9300
>     nodePort: 30930
>   selector:
>     app: elasticsearch
>   type: NodePort
> ```
>
> 위와 같이 작성 후 저장.
>
> > `namespace`는 필수는 아니지만, 관리 편의를 위해서 지정해서 묶음.
> >
> > 서비스는 NodePort로 꼭 할 필요는 없음. 필요에 따라서 로드밸런서를 써도 되고... 필요에 맞게 하면 된다.
>
> 네임스페이스 생성해줘야 한다.
>
> ```
> kubectl create ns elastic-stack
> ```
>
> 이 명령어로...
>
> ![image](https://user-images.githubusercontent.com/78403443/192928542-60b63485-71f6-4940-865b-e80152a4c9d1.png)
>
> 적용 후 확인. (아직 생성 중인 상황)
>
> ![image](https://user-images.githubusercontent.com/78403443/192928842-d4540120-7763-4b6d-b494-c9befba37bbd.png)
>
> 생성 완료 후 엔드포인트도 확인됨.
>
> ![image](https://user-images.githubusercontent.com/78403443/192928962-502d476a-48d4-4e77-af15-0078fa1a89fa.png)
>
> 30920 포트는 위와 같이 접속이 되고,
>
> ![image](https://user-images.githubusercontent.com/78403443/192929074-df73fc97-9a58-40d7-8e83-3e2c377fb144.png)
>
> 30930은 접속이 안된다.
>
> 이미지를 보면, 30920 포트로 접속한 것을 보면 뭔가 복잡하게 나온다... 이런 걸 예쁘게 보여주는게 kibana라는 툴 이다.
>
> `kibana.yaml` 파일 편집
>
> ```bash
> kind: Deployment
> metadata:
>   name: kibana
>   namespace: elastic-stack
> spec:
>   replicas: 3
>   selector:
>     matchLabels:
>       app: kibana
>   template:
>     metadata:
>       labels:
>         app: kibana
>     spec:
>       containers:
>       - name: kibana
>         image: elastic/kibana:6.4.0
>         env:
>         - name: SERVER_NAME
>           value: "kibana.kubernetes.example.com"
>         - name: ELASTICSEARCH_URL
>           value: "http://elasticsearch-svc.elastic-stack.svc.cluster.local:9200"
>         ports:
>         - containerPort: 5601
> 
> ---
> 
> apiVersion: v1
> kind: Service
> metadata:
>   name: kibana-svc
>   namespace: elastic-stack
> spec:
>   ports:
>   - name: kibana
>     port: 5601
>     targetPort: 5601
>     nodePort: 30561
>   selector:
>     app: kibana
>   type: NodePort
> ```
>
> 위와 같이 작성 후 저장.
>
> > 노드포트든 로드밸런서를 쓰든 상관 없음.
> >
> > 로드밸런서를 쓰면 바로 5601로 접근하면 될 것이다.
>
> 적용 후 확인 (아래)
>
> ![image](https://user-images.githubusercontent.com/78403443/192930253-38f4a95c-f4a3-4874-b73a-4b625d380558.png)
>
> 생성 완료 후 엔드포인트 확인 (아래)
>
> <192.168.56.11:30561> 로 웹 페이지 뜨는 거 확인.
>
> (시간 관계 상 접속이 오래걸려서 일단 건너 뜀.)
>
> > 아직 연결은 되지 않은 상태이다.
>
> 플루언트디를 이용해서 로그 수집해보자.
>
> `vim fluentd.yaml` 편집
>
> ```bash
> apiVersion: v1
> kind: ServiceAccount
> metadata:
>   name: fluentd
>   namespace: elastic-stack
> 
> ---
> 
> apiVersion: rbac.authorization.k8s.io/v1
> kind: ClusterRole
> metadata:
>   name: fluentd
>   namespace: elastic-stack
> rules:
> - apiGroups:
>   - ""
>   resources:
>   - pods
>   - namespaces
>   verbs:
>   - get
>   - list
>   - watch
> 
> ---
> 
> apiVersion: rbac.authorization.k8s.io/v1
> kind: ClusterRoleBinding
> metadata:
>   name: fluentd
>   namespace: elastic-stack
> roleRef:
>   kind: ClusterRole
>   name: fluentd
>   apiGroup: rbac.authorization.k8s.io
> subjects:
> - kind: ServiceAccount
>   name: fluentd
>   namespace: elastic-stack
> 
> ---
> 
> apiVersion: apps/v1
> kind: DaemonSet
> metadata:
>   name: fluentd
>   namespace: elastic-stack
> spec:
>   selector:
>     matchLabels:
>       k8s-app: fluentd-logging
>       version: v1
>   template:
>     metadata:
>       labels:
>         k8s-app: fluentd-logging
>         version: v1
>     spec:
>       serviceAccount: fluentd
>       serviceAccountName: fluentd
>       tolerations:
>       - key: node-role.kubernetes.io/master
>         effect: NoSchedule
>       containers:
>       - name: fluentd
>         image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
>         env:
>         - name: FLUENT_ELASTICSEARCH_HOST
>           value: "elasticsearch-svc.elastic-stack.svc.cluster.local"
>         - name: FLUENT_ELASTICSEARCH_PORT
>           value: "9200"
>         - name: FLUENT_ELASTICSEARCH_SCHEME
>           value: "http"
>         - name: FLUENT_UID
>           value: "0"
>         resources:
>           limits:
>             memory: 200Mi
>           requests:
>             cpu: 100m
>             memory: 200Mi
>         volumeMounts:
>         - name: varlog
>           mountPath: /var/log
>         - name: dockercontainerlogdirectory
>           mountPath: /var/lib/docker/containers
>           readOnly: true
>       terminationGracePeriodSeconds: 30
>       volumes:
>       - name: varlog
>         hostPath:
>           path: /var/log
>       - name: dockercontainerlogdirectory
>         hostPath:
>           path: /var/lib/docker/containers
> ```
>
> 위와 같이 작성 하고,
>
> `kubectl apply -f fluentd.yaml` 명령어로 적용
>
> `kubectl get po -n elastic-stack` 명령어로 파드 생성 확인.<br>(각자 환경마다 개수가 다르게 생성될 것임.)
>
> <192.168.56.11:30561> 에서
>
> ![image](https://user-images.githubusercontent.com/78403443/192934326-87e53701-8394-46e4-ae3b-5c420b9fd128.png)
>
> 패턴 생성 후 확인 (패턴에 따라서 데이터를 확인할 수 있다)

**쿠버네티스 대시보드**

- 기본적인 클러스터 관리는 `kubectl` 명령어 사용
- 그래픽UI 를 원하는 경우에는 대시보드 사용 가능<br>→ kubespray 배포 시에는 addone 으로 추가 구성 가능<br>(+ 사용자 권한설정 필요)<br>github 을 통해 필요한 파일을 다운로드/설치<br>네트워크 및 그래픽환경 구성이 기본 요구

> ```
> wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
> ```
>
> ![image](https://user-images.githubusercontent.com/78403443/192945287-66b20ab4-e56b-4d0a-8d51-fd068101fc7f.png)
>
> 위 명령어로 다운 후,<br>`/logging/recommended.yaml` 파일 열어봄.
>
> ![image](https://user-images.githubusercontent.com/78403443/192945694-cfb800a5-ad30-4520-82c1-3c8a19eb0fff.png)

**쿠버네티스 모니터링**

> metrics 서버가 배포 되있으면 `kubectl top node`, `kubectl top pod` 명령어로 간단하게 성능 모니터링을 할 수 있다.
>
> <https://github.com/kubernetes-sigs/metrics-server> 이 페이지에서 메트릭스 설치 부분 참고.
>
> > ```
> > kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
> > ```
> >
> > 이 명령어로 설치
>
> 오류 나면,
>
> ```
> wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
> ```
>
> `vi components.yaml` 편집
>
> ![image](https://user-images.githubusercontent.com/78403443/192952906-34c7cad7-14e9-43c7-ba49-9aaa98c76f17.png)
>
> 추가 후 
>
> `kubectl apply -f components.yaml` 명령어로 적용.
>
> ---
>
> 프로메테우스
>
> `vi /logging/prometheus.yaml` 파일 편집
>
> > <https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus-kubernetes.yml> 참고
>
> ```bash
> # A scrape configuration for running Prometheus on a Kubernetes cluster.
> # This uses separate scrape configs for cluster components (i.e. API server, node)
> # and services to allow each to use different authentication configs.
> #
> # Kubernetes labels will be added as Prometheus labels on metrics via the
> # `labelmap` relabeling action.
> #
> # If you are using Kubernetes 1.7.2 or earlier, please take note of the comments
> # for the kubernetes-cadvisor job; you will need to edit or remove this job.
> 
> # Scrape config for API servers.
> #
> # Kubernetes exposes API servers as endpoints to the default/kubernetes
> # service so this uses `endpoints` role and uses relabelling to only keep
> # the endpoints associated with the default/kubernetes service using the
> # default named port `https`. This works for single API server deployments as
> # well as HA API server deployments.
> scrape_configs:
>   - job_name: "kubernetes-apiservers"
> 
>     kubernetes_sd_configs:
>       - role: endpoints
> 
>     # Default to scraping over https. If required, just disable this or change to
>     # `http`.
>     scheme: https
> 
>     # This TLS & authorization config is used to connect to the actual scrape
>     # endpoints for cluster components. This is separate to discovery auth
>     # configuration because discovery & scraping are two separate concerns in
>     # Prometheus. The discovery auth config is automatic if Prometheus runs inside
>     # the cluster. Otherwise, more config options have to be provided within the
>     # <kubernetes_sd_config>.
>     tls_config:
>       ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
>       # If your node certificates are self-signed or use a different CA to the
>       # master CA, then disable certificate verification below. Note that
>       # certificate verification is an integral part of a secure infrastructure
>       # so this should only be disabled in a controlled environment. You can
>       # disable certificate verification by uncommenting the line below.
>       #
>       # insecure_skip_verify: true
>     authorization:
>       credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
> 
>     # Keep only the default/kubernetes service endpoints for the https port. This
>     # will add targets for each API server which Kubernetes adds an endpoint to
>     # the default/kubernetes service.
>     relabel_configs:
>       - source_labels:
>           [
>             __meta_kubernetes_namespace,
>             __meta_kubernetes_service_name,
>             __meta_kubernetes_endpoint_port_name,
>           ]
>         action: keep
>         regex: default;kubernetes;https
> 
>   # Scrape config for nodes (kubelet).
>   #
>   # Rather than connecting directly to the node, the scrape is proxied though the
>   # Kubernetes apiserver.  This means it will work if Prometheus is running out of
>   # cluster, or can't connect to nodes for some other reason (e.g. because of
>   # firewalling).
>   - job_name: "kubernetes-nodes"
> 
>     # Default to scraping over https. If required, just disable this or change to
>     # `http`.
>     scheme: https
> 
>     # This TLS & authorization config is used to connect to the actual scrape
>     # endpoints for cluster components. This is separate to discovery auth
>     # configuration because discovery & scraping are two separate concerns in
>     # Prometheus. The discovery auth config is automatic if Prometheus runs inside
>     # the cluster. Otherwise, more config options have to be provided within the
>     # <kubernetes_sd_config>.
>     tls_config:
>       ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
>       # If your node certificates are self-signed or use a different CA to the
>       # master CA, then disable certificate verification below. Note that
>       # certificate verification is an integral part of a secure infrastructure
>       # so this should only be disabled in a controlled environment. You can
>       # disable certificate verification by uncommenting the line below.
>       #
>       # insecure_skip_verify: true
>     authorization:
>       credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
> 
>     kubernetes_sd_configs:
>       - role: node
> 
>     relabel_configs:
>       - action: labelmap
>         regex: __meta_kubernetes_node_label_(.+)
>       - target_label: __address__
>         replacement: kubernetes.default.svc:443
>       - source_labels: [__meta_kubernetes_node_name]
>         regex: (.+)
>         target_label: __metrics_path__
>         replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
> 
>   # Scrape config for Kubelet cAdvisor.
>   #
>   # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
>   # (those whose names begin with 'container_') have been removed from the
>   # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
>   # retrieve those metrics.
>   #
>   # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
>   # HTTP endpoint; use the "/metrics" endpoint on the 4194 port of nodes. In
>   # that case (and ensure cAdvisor's HTTP server hasn't been disabled with the
>   # --cadvisor-port=0 Kubelet flag).
>   #
>   # This job is not necessary and should be removed in Kubernetes 1.6 and
>   # earlier versions, or it will cause the metrics to be scraped twice.
>   - job_name: "kubernetes-cadvisor"
> 
>     # Default to scraping over https. If required, just disable this or change to
>     # `http`.
>     scheme: https
> 
>     # Starting Kubernetes 1.7.3 the cAdvisor metrics are under /metrics/cadvisor.
>     # Kubernetes CIS Benchmark recommends against enabling the insecure HTTP
>     # servers of Kubernetes, therefore the cAdvisor metrics on the secure handler
>     # are used.
>     metrics_path: /metrics/cadvisor
> 
>     # This TLS & authorization config is used to connect to the actual scrape
>     # endpoints for cluster components. This is separate to discovery auth
>     # configuration because discovery & scraping are two separate concerns in
>     # Prometheus. The discovery auth config is automatic if Prometheus runs inside
>     # the cluster. Otherwise, more config options have to be provided within the
>     # <kubernetes_sd_config>.
>     tls_config:
>       ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
>       # If your node certificates are self-signed or use a different CA to the
>       # master CA, then disable certificate verification below. Note that
>       # certificate verification is an integral part of a secure infrastructure
>       # so this should only be disabled in a controlled environment. You can
>       # disable certificate verification by uncommenting the line below.
>       #
>       # insecure_skip_verify: true
>     authorization:
>       credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
> 
>     kubernetes_sd_configs:
>       - role: node
> 
>     relabel_configs:
>       - action: labelmap
>         regex: __meta_kubernetes_node_label_(.+)
>       - target_label: __address__
>         replacement: kubernetes.default.svc:443
>       - source_labels: [__meta_kubernetes_node_name]
>         regex: (.+)
>         target_label: __metrics_path__
>         replacement: /api/v1/nodes/${1}/proxy/metrics
> 
>   # Example scrape config for service endpoints.
>   #
>   # The relabeling allows the actual service scrape endpoint to be configured
>   # for all or only some endpoints.
>   - job_name: "kubernetes-service-endpoints"
> 
>     kubernetes_sd_configs:
>       - role: endpoints
> 
>     relabel_configs:
>       # Example relabel to scrape only endpoints that have
>       # "example.io/should_be_scraped = true" annotation.
>       #  - source_labels: [__meta_kubernetes_service_annotation_example_io_should_be_scraped]
>       #    action: keep
>       #    regex: true
>       #
>       # Example relabel to customize metric path based on endpoints
>       # "example.io/metric_path = <metric path>" annotation.
>       #  - source_labels: [__meta_kubernetes_service_annotation_example_io_metric_path]
>       #    action: replace
>       #    target_label: __metrics_path__
>       #    regex: (.+)
>       #
>       # Example relabel to scrape only single, desired port for the service based
>       # on endpoints "example.io/scrape_port = <port>" annotation.
>       #  - source_labels: [__address__, __meta_kubernetes_service_annotation_example_io_scrape_port]
>       #    action: replace
>       #    regex: ([^:]+)(?::\d+)?;(\d+)
>       #    replacement: $1:$2
>       #    target_label: __address__
>       #
>       # Example relabel to configure scrape scheme for all service scrape targets
>       # based on endpoints "example.io/scrape_scheme = <scheme>" annotation.
>       #  - source_labels: [__meta_kubernetes_service_annotation_example_io_scrape_scheme]
>       #    action: replace
>       #    target_label: __scheme__
>       #    regex: (https?)
>       - action: labelmap
>         regex: __meta_kubernetes_service_label_(.+)
>       - source_labels: [__meta_kubernetes_namespace]
>         action: replace
>         target_label: namespace
>       - source_labels: [__meta_kubernetes_service_name]
>         action: replace
>         target_label: service
> 
>   # Example scrape config for probing services via the Blackbox Exporter.
>   #
>   # The relabeling allows the actual service scrape endpoint to be configured
>   # for all or only some services.
>   - job_name: "kubernetes-services"
> 
>     metrics_path: /probe
>     params:
>       module: [http_2xx]
> 
>     kubernetes_sd_configs:
>       - role: service
> 
>     relabel_configs:
>       # Example relabel to probe only some services that have "example.io/should_be_probed = true" annotation
>       #  - source_labels: [__meta_kubernetes_service_annotation_example_io_should_be_probed]
>       #    action: keep
>       #    regex: true
>       - source_labels: [__address__]
>         target_label: __param_target
>       - target_label: __address__
>         replacement: blackbox-exporter.example.com:9115
>       - source_labels: [__param_target]
>         target_label: instance
>       - action: labelmap
>         regex: __meta_kubernetes_service_label_(.+)
>       - source_labels: [__meta_kubernetes_namespace]
>         target_label: namespace
>       - source_labels: [__meta_kubernetes_service_name]
>         target_label: service
> 
>   # Example scrape config for probing ingresses via the Blackbox Exporter.
>   #
>   # The relabeling allows the actual ingress scrape endpoint to be configured
>   # for all or only some services.
>   - job_name: "kubernetes-ingresses"
> 
>     metrics_path: /probe
>     params:
>       module: [http_2xx]
> 
>     kubernetes_sd_configs:
>       - role: ingress
> 
>     relabel_configs:
>       # Example relabel to probe only some ingresses that have "example.io/should_be_probed = true" annotation
>       #  - source_labels: [__meta_kubernetes_ingress_annotation_example_io_should_be_probed]
>       #    action: keep
>       #    regex: true
>       - source_labels:
>           [
>             __meta_kubernetes_ingress_scheme,
>             __address__,
>             __meta_kubernetes_ingress_path,
>           ]
>         regex: (.+);(.+);(.+)
>         replacement: ${1}://${2}${3}
>         target_label: __param_target
>       - target_label: __address__
>         replacement: blackbox-exporter.example.com:9115
>       - source_labels: [__param_target]
>         target_label: instance
>       - action: labelmap
>         regex: __meta_kubernetes_ingress_label_(.+)
>       - source_labels: [__meta_kubernetes_namespace]
>         target_label: namespace
>       - source_labels: [__meta_kubernetes_ingress_name]
>         target_label: ingress
> 
>   # Example scrape config for pods
>   #
>   # The relabeling allows the actual pod scrape to be configured
>   # for all the declared ports (or port-free target if none is declared)
>   # or only some ports.
>   - job_name: "kubernetes-pods"
> 
>     kubernetes_sd_configs:
>       - role: pod
> 
>     relabel_configs:
>       # Example relabel to scrape only pods that have
>       # "example.io/should_be_scraped = true" annotation.
>       #  - source_labels: [__meta_kubernetes_pod_annotation_example_io_should_be_scraped]
>       #    action: keep
>       #    regex: true
>       #
>       # Example relabel to customize metric path based on pod
>       # "example.io/metric_path = <metric path>" annotation.
>       #  - source_labels: [__meta_kubernetes_pod_annotation_example_io_metric_path]
>       #    action: replace
>       #    target_label: __metrics_path__
>       #    regex: (.+)
>       #
>       # Example relabel to scrape only single, desired port for the pod
>       # based on pod "example.io/scrape_port = <port>" annotation.
>       #  - source_labels: [__address__, __meta_kubernetes_pod_annotation_example_io_scrape_port]
>       #    action: replace
>       #    regex: ([^:]+)(?::\d+)?;(\d+)
>       #    replacement: $1:$2
>       #    target_label: __address__
>       - action: labelmap
>         regex: __meta_kubernetes_pod_label_(.+)
>       - source_labels: [__meta_kubernetes_namespace]
>         action: replace
>         target_label: namespace
>       - source_labels: [__meta_kubernetes_pod_name]
>         action: replace
>         target_label: pod
> ```
>
> 위와 같이 작성하고,
>
> ```
> kubectl create configmap prometheus-kubernetes --from-file=./prometheus.yaml
> ```
>
> 위 명령어로 적용
>
> `vim /logging/prometheus-deploy.yaml` 편집
>
> ```bash
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: prometheus-app
> spec:
>   replicas: 1
>   selector:
>     matchLabels:
>       app: prometheus-app
>   template:
>     metadata:
>       labels:
>         app: prometheus-app
>     spec:
>       containers:
>       - name: prometheus-app
>         image: prom/prometheus:v2.27.1
>         args:
>         - "--config.file=/etc/prometheus/prometheus-kubernetes-config.yaml"
>         ports:
>         - containerPort: 9090
>         volumeMounts:
>         - name: config-volume
>           mountPath: /etc/prometheus
>         - name: storage-volume
>           mountPath: /prometheus/
>       volumes:
>       - name: config-volume
>         configMap:
>           name: prometheus-kubernetes
>       - name: storage-volume
>         emptyDir: {}
> 
> ---
> apiVersion: v1
> kind: Service
> metadata:
>   name: prometheus-app
> spec:
>   ports:
>   - nodePort: 30990
>     port: 9090
>     targetPort: 9090
>   selector:
>     app: prometheus-app
>   type: NodePort
> ```
>
> 위와 같이 접속 후
>
> ```
> kubectl apply -f prometheus-deploy.yaml
> ```
>
> 이 명령어로 적용
>
> <192.168.56.11:30990> 브라우저에서 접속해서 확인. (교재 p.422 참고)
>
> `vi grafana.yaml` 파일 작성
>
> ```bash
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: grafana-app
> spec:
>   selector:
>     matchLabels:
>       k8s-app: grafana
>   replicas: 1
>   template:
>     metadata:
>       labels:
>         k8s-app: grafana
>     spec:
>       containers:
>       - name: grafana
>         image: grafana/grafana:8.0.2
>         ports:
>         - containerPort: 3000
>         env:
>         - name: GF_SERVER_HTTP_PORT
>           value: "3000"
>         - name: GF_AUTH_BASIC_ENABLED
>           value: "false"
>         - name: GF_AUTH_ANONYMOUS_ENABLED
>           value: "true"
>         - name: GF_AUTH_ANONYMOUS_ORG_ROLE
>           value: Admin
>         - name: GF_SERVER_ROOT_URL
>           value: /
> ---
> apiVersion: v1
> kind: Service
> metadata:
>   name: grafana-svc
> spec:
>   selector:
>       k8s-app: grafana
>   ports:
>   - port: 3000
>     targetPort: 3000
>     nodePort: 30300
>   type: NodePort
> ```
>
> 위와 같이 작성 후,
>
> ```
> kubectl apply -f grafana.yaml
> ```
>
> 이 명령어로 적용
>
> ```
> kubectl get svc
> ```
>
> 명령어로 확인한 후 해당 포트로 접속 확인.

쿠버네티스 모니터링

- 기본적으로 모니터링 기능은 지원 X

- 리소스 사용량에 대한 모니터링을 위해서<br>→ metric 서버 구성이 필요

  1. kubespray 배포 시 addons 항목으로 추가 구성
  2. 별도로 배포 후 추가 구성도 가능

  metric 서버 구성 시 kubectl top 명령어 확인

- 기본적인 제공
  - kubelet 컴포넌트에 cAdvisor 도구 포함 (메모리)

- 별도로 metric 정보 모니터링 도구인 prometheus 설치
- 시각화를 위해 grafana 추가 구성 가능

---

**오토스케일링** (교재 p.434 부터)

> 참고 : <https://kubernetes.io/ko/docs/tasks/run-application/horizontal-pod-autoscale/>

- 쿠버네티스 환경에서는 파드를 통해 사용자에게 서비스 제공
- 파드의 개수에 따라 리소스 및 엔드포인트 증가<br>→ 서비스 제공이 원활
- 사용자 편의를 위해서는 파드 개수가 많을 수록 좋음
- 물리적인 리소스의 제약으로 인해 파드 개수도 제한
- 사용량에 따라 자동으로 파드 개수를 조정<br>→  HPA

> ![image](https://user-images.githubusercontent.com/78403443/192968739-63371394-4384-43d0-b546-c371961454dd.png)
>
> 교재는 1 버전이지만, 우리는 지금 2 버전으로 차이가 있다.
